{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../utils/utils.py\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "class DataLoader(object):\n",
    "    @staticmethod\n",
    "    def load_data_label(path: str):\n",
    "        \"\"\"\n",
    "        this is for input file with (coordinate_x, coordinate_y, ... , label) in each line\n",
    "        \"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            data = []\n",
    "            label = []\n",
    "            for l in f.readlines():\n",
    "                source = l.strip().split()\n",
    "                data.append([float(val) for val in source[:-1]])\n",
    "                label.append(int(source[-1]))\n",
    "            return np.array(data), np.array(label)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_data(path: str):\n",
    "        \"\"\"\n",
    "        this is for input file with (coordinate_x, coordinate_y, ...) in each line\n",
    "        \"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            data = []\n",
    "            for l in f.readlines():\n",
    "                source = l.strip().split()\n",
    "                data.append([float(val) for val in source])\n",
    "            return np.array(data)\n",
    "\n",
    "\n",
    "class Evaluation(object):\n",
    "    @classmethod\n",
    "    def silhouette_coefficient(cls, dbscan_obj):\n",
    "        def a(pid, tags, dist_matrix):\n",
    "            mask = tags == tags[pid]\n",
    "            avg_dist = np.sum(dist_matrix[pid] * mask, axis=0) / np.sum(mask)\n",
    "            return avg_dist\n",
    "\n",
    "        def b(pid, tags, dist_matrix):\n",
    "            avg_dists = []\n",
    "            for label in range(1,\n",
    "                               max(tags) + 1):  # cluster label starts from 1\n",
    "                if label == tags[pid]:\n",
    "                    continue\n",
    "                mask = tags == label\n",
    "                avg_dists.append(\n",
    "                    np.sum(dist_matrix[pid] * mask, axis=0) / np.sum(mask))\n",
    "            return min(avg_dists)\n",
    "\n",
    "        # preparation\n",
    "        # if sum(dbscan_obj.tags) == -dbscan_obj.num_p:\n",
    "        if sum(dbscan_obj.tags) < 0:\n",
    "            raise Exception('There are no tags in target dbscan method.')\n",
    "        if not hasattr(dbscan_obj, 'dist_m'):\n",
    "            # by default, we try to use matrix dbscan to tune parameters\n",
    "            # BUG: If use basic dbscan has no _get_distance_matrix() attribute function\n",
    "            dbscan_obj._get_distance_matrix()\n",
    "        tags = np.array(dbscan_obj.tags)\n",
    "\n",
    "        # TODO: this method still can be optimised by matrix computation\n",
    "        res = 0\n",
    "        for pid in range(dbscan_obj.num_p):\n",
    "            tmp_a = a(pid, tags, dbscan_obj.dist_m)\n",
    "            tmp_b = b(pid, tags, dbscan_obj.dist_m)\n",
    "            res += (tmp_b - tmp_a) / max(tmp_b, tmp_a)\n",
    "        res /= dbscan_obj.num_p\n",
    "\n",
    "        print(\n",
    "            f'eps: {dbscan_obj.eps} min points: {dbscan_obj.min_pts} silhouette coefficient: {res}'\n",
    "        )\n",
    "        return res\n",
    "\n",
    "\n",
    "def timeit(func):\n",
    "    def wrapper(*args, **wargs):\n",
    "        start = time.time()\n",
    "        res = func(*args, **wargs)\n",
    "        end = time.time()\n",
    "        print(f'{func.__name__} time cost: {(end-start)*1000}ms')\n",
    "        return res\n",
    "\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local DBSCAN Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Status\n",
    "UNKNOWN = -1\n",
    "NOISE = -2\n",
    "\n",
    "class DBSCAN(object):\n",
    "    \"\"\"\n",
    "    Base Class of DBSCAN, please do NOT instantiate this Class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"\n",
    "        DBSCAN Classes should be instantiate with data point set\n",
    "        \"\"\"\n",
    "        self.m, _ = (dataset, None)     # placeholder _ for future implementation of labels\n",
    "        self.num_p = self.m.shape[0]\n",
    "        self.tags = [UNKNOWN] * self.num_p\n",
    "\n",
    "    def _get_dist(self, a, b, fast_mode: bool = False) -> float:\n",
    "        \"\"\"\n",
    "        for float comparison, set all distance value precision to 5\n",
    "        :param: a: int; index of given point in data matrix\n",
    "        :param: b: same as a\n",
    "        :param: fast_mode: bool -> if True, ignore sqrt() opration for distance\n",
    "        \"\"\"\n",
    "        if fast_mode:\n",
    "            result = np.power(self.m[b] - self.m[a], 2).sum()\n",
    "        else:\n",
    "            result = np.sqrt(np.power(self.m[b] - self.m[a], 2).sum())\n",
    "        return round(result, 5)\n",
    "\n",
    "    def _get_neighbours(self, p: int, eps: float, fast_mode=False) -> list:\n",
    "        \"\"\"\n",
    "        return neighbours index of given point p in source data matrix\n",
    "        :param: p: int; index of given point in data matrix\n",
    "        :param: eps: float; the value of radius of density area\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _clustering(self, p, eps, min_pts, cluster_id, fast_mode=False):\n",
    "        \"\"\"\n",
    "        tag given point p and all of its neighbours and sub-neighbours with the same cluster id\n",
    "        :param: m: np.matrix; N * 2 matrix recoding all nodes' coordinates\n",
    "        :param: eps: float; the value of radius of density area\n",
    "        :param: min_pts: int; least neighbours should be in a density area\n",
    "        :param: cluster_id: int; current id of cluster\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _find_core_pts(self, eps, min_pts):\n",
    "        self.is_core = [0] * self.num_p\n",
    "        for i in range(self.num_p):\n",
    "            if len(self._get_neighbours(i, eps, min_pts)) > min_pts:\n",
    "                self.is_core[i] = 1\n",
    "        return self.is_core\n",
    "        \n",
    "\n",
    "    def predict(self, eps, min_pts, fast_mode=False) -> list:\n",
    "        \"\"\"\n",
    "        return list of labels as the sequence in data matrix\n",
    "        :param: m: np.matrix; N * 2 matrix recoding all nodes' coordinates\n",
    "        :param: eps: float; the value of radius of density area\n",
    "        :param: min_pts: int; least neighbours should be in a density area\n",
    "        \"\"\"\n",
    "        self.eps = eps\n",
    "        self.min_pts = min_pts\n",
    "\n",
    "        cluster_id = 1\n",
    "        for p_id in range(self.num_p):\n",
    "            if self.tags[p_id] != UNKNOWN:\n",
    "                continue\n",
    "            if self._clustering(p_id, eps, min_pts, cluster_id, fast_mode):\n",
    "                cluster_id += 1\n",
    "        return np.array(self.tags)\n",
    "\n",
    "\n",
    "class NaiveDBSCAN(DBSCAN):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        super(NaiveDBSCAN, self).__init__(dataset)\n",
    "\n",
    "    def _get_neighbours(self, p: int, eps: float, fast_mode=False) -> list:\n",
    "\n",
    "        ngbs = []\n",
    "        for idx in range(len(self.m)):\n",
    "            if self._get_dist(p, idx, fast_mode) < eps:\n",
    "                ngbs.append(idx)\n",
    "        return ngbs\n",
    "\n",
    "    def _clustering(self, p, eps, min_pts, cluster_id, fast_mode=False) -> bool:\n",
    "\n",
    "        neighbours = self._get_neighbours(p, eps, fast_mode)\n",
    "        if len(neighbours) < min_pts:\n",
    "            self.tags[p] = NOISE\n",
    "            return False\n",
    "        else:\n",
    "            self.tags[p] = cluster_id\n",
    "            for idx in neighbours:\n",
    "                self.tags[idx] = cluster_id\n",
    "            while len(neighbours) > 0:\n",
    "                sub_neighbours = self._get_neighbours(neighbours[0], eps, fast_mode)\n",
    "                if len(sub_neighbours) >= min_pts:\n",
    "                    for sub_n in sub_neighbours:\n",
    "                        if self.tags[sub_n] < 0:\n",
    "                            self.tags[sub_n] = cluster_id\n",
    "                            if self.tags[sub_n] == UNKNOWN:\n",
    "                                neighbours.append(sub_n)\n",
    "                neighbours = neighbours[1:]\n",
    "        return True\n",
    "    \n",
    "\n",
    "class MatrixDBSCAN(DBSCAN):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        super(MatrixDBSCAN, self).__init__(dataset)\n",
    "        self._get_distance_matrix()     # self.dist_m will be created\n",
    "        del self.m\n",
    "\n",
    "    def _get_distance_matrix(self):\n",
    "        \"\"\"\n",
    "        Only once calculation will be on each point-pairs\n",
    "        results will be stored in self.dist_m\n",
    "        \"\"\"\n",
    "\n",
    "        self.dist_m = np.zeros((self.num_p, self.num_p))\n",
    "        for p_id in range(self.num_p):\n",
    "            for q_id in range(p_id, self.num_p):\n",
    "                dist = self._get_dist(p_id, q_id)\n",
    "                self.dist_m[q_id, p_id] = dist\n",
    "                self.dist_m[p_id, q_id] = dist\n",
    "\n",
    "    def _get_neighbours(self, p: int, eps: float, fast_mode=False) -> list:\n",
    "        return np.nonzero(self.dist_m[p] < eps)[0]\n",
    "\n",
    "    def _clustering(self, p, eps, min_pts, cluster_id, fast_mode=False) -> bool:\n",
    "        \"\"\"\n",
    "        TODO: There should be some optimizations for this part, current code is too ugly\n",
    "        \"\"\"\n",
    "\n",
    "        neighbours = self._get_neighbours(p, eps, fast_mode)\n",
    "        if len(neighbours) < min_pts:\n",
    "            self.tags[p] = NOISE\n",
    "            return False\n",
    "        else:\n",
    "            self.tags[p] = cluster_id\n",
    "            for idx in neighbours:\n",
    "                self.tags[idx] = cluster_id\n",
    "            while len(neighbours) > 0:\n",
    "                sub_neighbours = self._get_neighbours(neighbours[0], eps, fast_mode)\n",
    "                if len(sub_neighbours) >= min_pts:\n",
    "                    for sub_n in sub_neighbours:\n",
    "                        if self.tags[sub_n] < 0:\n",
    "                            self.tags[sub_n] = cluster_id\n",
    "                            if self.tags[sub_n] == UNKNOWN:\n",
    "                                neighbours.append(sub_n)\n",
    "                neighbours = neighbours[1:]\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark import RDD\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(l):\n",
    "    s = l.split()\n",
    "    return (float(s[0]), float(s[1]))\n",
    "def load_data_label(path):\n",
    "    pts = sc.textFile(path).map(parse_line)\n",
    "    return pts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bounds_coordinates(bin_bounds):\n",
    "#     coordinates = [bounds[0]]\n",
    "\n",
    "    lower_cdnts = [[low] for low in  bin_bounds[0][:-1]]\n",
    "    upper_cdnts = [[high] for high in bin_bounds[0][1:]]\n",
    "    \n",
    "    # super stupid implementation, optimization needed\n",
    "    for bound in bin_bounds[1:]:\n",
    "        lower_tmp = []\n",
    "        upper_tmp = []\n",
    "        \n",
    "        for bc in bound[:-1]:\n",
    "            lower_tmp.extend([lc + [bc] for lc in lower_cdnts])\n",
    "        lower_cdnts = lower_tmp\n",
    "        \n",
    "        for bc in bound[1:]:\n",
    "            upper_tmp.extend([uc + [bc] for uc in upper_cdnts])\n",
    "        upper_cdnts = upper_tmp\n",
    "        \n",
    "    return np.array(lower_cdnts), np.array(upper_cdnts)\n",
    "\n",
    "def partition(dataset, n_partitions, eps):\n",
    "    \"\"\"\n",
    "    n_partitions: tuple with shape correspoding to dataset dimension\n",
    "    \"\"\"\n",
    "    partition_dim = n_partitions\n",
    "    n_partitions = np.prod(n_partitions)\n",
    "    # cut bins\n",
    "    bounds = np.concatenate(([np.min(dataset, axis=0)], [np.max(dataset, axis=0)]), axis=0) # 2 * D\n",
    "    bounds_dim = bounds.T # D * 2, \n",
    "    \n",
    "    bin_bounds = []\n",
    "    for i in range(len(partition_dim)):\n",
    "        dim_bins = np.linspace(*bounds_dim[i], partition_dim[i]+1, endpoint=True)\n",
    "        bin_bounds.append(dim_bins)\n",
    "    \n",
    "    lower_bounds, upper_bounds = _bounds_coordinates(bin_bounds)\n",
    "    lower_bounds -= eps\n",
    "    upper_bounds += eps\n",
    "    \n",
    "    print(lower_bounds)\n",
    "    print(upper_bounds)\n",
    "    \n",
    "    # scatter points into bins with eps\n",
    "    indexed_data = []\n",
    "    for id_pts in range(len(dataset)):     # index of point in dataset\n",
    "        for id_ptt in range(n_partitions):\n",
    "            if not (dataset[id_pts] > lower_bounds[id_ptt]).all():\n",
    "                continue\n",
    "            if not (dataset[id_pts] < upper_bounds[id_ptt]).all():\n",
    "                continue\n",
    "            indexed_data.append([id_ptt, id_pts])\n",
    "            \n",
    "    res = sc.parallelize(indexed_data).groupByKey().map(lambda x: [x[0], list(x[1])])\n",
    "    return res\n",
    "\n",
    "def local_dbscan(partioned_rdd):\n",
    "\n",
    "    dataset = np.array([b_dataset.value[idp] for idp in partioned_rdd])\n",
    "    dbscan_obj = MatrixDBSCAN(dataset)\n",
    "    dbscan_obj.predict(b_eps.value, b_min_pts.value)\n",
    "    is_core_list = dbscan_obj._find_core_pts(b_eps.value, b_min_pts.value)\n",
    "    \n",
    "    return list(zip(zip(partioned_rdd, is_core_list), dbscan_obj.tags))\n",
    "\n",
    "def merge(local_tags, dataset):\n",
    "    global_tags = [UNKNOWN] * len(dataset)\n",
    "    is_tagged = [0] * len(dataset)\n",
    "    last_max_label = 0\n",
    "    for local in local_tags:\n",
    "        np_local = np.array(local[-1])\n",
    "        np_local[:, -1] += last_max_label\n",
    "        last_max_label = np.max(np_local[:, -1])\n",
    "        \n",
    "        # check and merge overlapped points\n",
    "        tagged_indices = np.nonzero(is_tagged)[0]\n",
    "        for tmp_i in range(len(np_local)):\n",
    "            # should do tag check\n",
    "            (p_id, is_core), label = np_local[tmp_i]\n",
    "            if p_id in tagged_indices and is_core==1:\n",
    "                np_local[-1][np_local[-1]==label] = global_tags[p_id]\n",
    "        \n",
    "        # update global tags\n",
    "        for (p_id, is_core), label in np_local:\n",
    "            if is_tagged[p_id]==1:\n",
    "                continue\n",
    "            global_tags[p_id] = label\n",
    "            is_tagged[p_id] = 1\n",
    "    return global_tags\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = '../../s1.txt'\n",
    "dataset = load_data_label(test_file)\n",
    "n_partitions = 4\n",
    "eps = 0.8\n",
    "min_pts = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rtree import index\n",
    "from collections import deque\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 19834.2         51120.2       ]\n",
      " [333872.86666667  51120.2       ]\n",
      " [647911.53333333  51120.2       ]\n",
      " [ 19834.2        510937.7       ]\n",
      " [333872.86666667 510937.7       ]\n",
      " [647911.53333333 510937.7       ]]\n",
      "[[333874.46666667 510939.3       ]\n",
      " [647913.13333333 510939.3       ]\n",
      " [961951.8        510939.3       ]\n",
      " [333874.46666667 970756.8       ]\n",
      " [647913.13333333 970756.8       ]\n",
      " [961951.8        970756.8       ]]\n"
     ]
    }
   ],
   "source": [
    "b_dataset = sc.broadcast(dataset)\n",
    "b_eps = sc.broadcast(eps)\n",
    "b_min_pts = sc.broadcast(min_pts)\n",
    "n_partitions = (3, 2)\n",
    "\n",
    "partitioned_rdd = partition(dataset, n_partitions, eps)\n",
    "local_tags = partitioned_rdd.mapValues(lambda x: local_dbscan(x)).collect()\n",
    "result_tags = merge(local_tags, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 580\n",
      "1 1092\n",
      "2 749\n",
      "3 807\n",
      "4 823\n",
      "5 949\n"
     ]
    }
   ],
   "source": [
    "for i in partitioned_rdd.collect():\n",
    "    print(i[0], len(i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liyangling/Envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:50: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps: 0.8 min points: 10 silhouette coefficient: 0.5871332307631024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5871332307631024"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan_obj = MatrixDBSCAN(np.array(dataset))\n",
    "dbscan_obj.tags = result_tags\n",
    "dbscan_obj.eps = eps\n",
    "dbscan_obj.min_pts = min_pts\n",
    "\n",
    "Evaluation.silhouette_coefficient(dbscan_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RTree based Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rtree import index\n",
    "from collections import deque\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_base_partition(rtree, maxCost, eps):\n",
    "    mbr = rtree.bounds\n",
    "    partition_list = []\n",
    "    queue = deque()\n",
    "    queue.append(mbr)\n",
    "    while len(queue):\n",
    "        br = queue.popleft()\n",
    "        nPoints = rtree.count(br)\n",
    "        if _get_cost(br, nPoints)> maxCost:\n",
    "            (subbr1, subbr2) = _cost_base_split(rtree, br, eps)\n",
    "            queue.append(subbr1)\n",
    "            queue.append(subbr2)\n",
    "        else:\n",
    "            partition_list.append(br)\n",
    "    return partition_list\n",
    "\n",
    "def _get_cost(bounds, nPoints, fanout = 2):\n",
    "    h = math.log( (nPoints+1) /fanout, fanout) + 1\n",
    "    DA  = h + math.sqrt(nPoints)*2/(math.sqrt(fanout) - 1) + nPoints/(fanout - 1) + 1\n",
    "    return DA * nPoints\n",
    "\n",
    "\n",
    "def _cost_base_split(rtree, bounds, eps):\n",
    "    (xmin, ymin, xmax, ymax) = bounds\n",
    "    #vertical split  \n",
    "    ymin_diff = float('inf')    \n",
    "    ysplit = ymin + (ymax - ymin)/2\n",
    "    ybest_split = ((xmin, ymin, xmax, ysplit), (xmin, ysplit, xmax, ymax)) \n",
    "    while( ysplit + eps * 2<= ymax):      \n",
    "        lowerbr = (xmin, ymin, xmax, ysplit)\n",
    "        lowercost = _get_cost(lowerbr, rtree.count(lowerbr))\n",
    "        \n",
    "        upperbr = (xmin, ysplit, xmax, ymax)\n",
    "        uppercost = _get_cost(upperbr, rtree.count(upperbr))\n",
    "        costdiff = abs(uppercost - lowercost)\n",
    "        if costdiff < ymin_diff:\n",
    "            ymin_diff = costdiff\n",
    "            ybest_split = (lowerbr, upperbr)\n",
    "            if uppercost < lowercost:\n",
    "                ysplit = ymin + (ysplit - ymin)/2\n",
    "            else:\n",
    "                ysplit = ysplit + (ymax - ysplit)/2\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    #horizontal split\n",
    "    xmin_diff = float('inf')    \n",
    "    xsplit = xmin + (xmax - xmin)/2\n",
    "    xbest_split = ((xmin, ymin, xsplit, ysplit), (xsplit, ymin, xmax, ymax))\n",
    "    while( xsplit + eps * 2<= xmax):   \n",
    "        lowerbr = (xmin, ymin, xsplit, ymax)\n",
    "        lowercost = _get_cost(lowerbr, rtree.count(lowerbr))\n",
    "        \n",
    "        upperbr = (xsplit, ymin, xmax, ymax)\n",
    "        uppercost = _get_cost(upperbr, rtree.count(upperbr))\n",
    "        costdiff = abs(uppercost - lowercost)\n",
    "        if costdiff < xmin_diff:\n",
    "            xmin_diff = costdiff\n",
    "            xbest_split = (lowerbr, upperbr)\n",
    "            if uppercost < lowercost:\n",
    "                xsplit = xmin + (xsplit - xmin)/2\n",
    "            else:\n",
    "                xsplit = xsplit + (xmax - xsplit)/2        \n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    #compare ysplit and xsplit\n",
    "    if xmin_diff < ymin_diff:\n",
    "        return xbest_split\n",
    "    else:\n",
    "        return ybest_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduced_boundary_partition(rtree, maxPoints, eps):\n",
    "    mbr = rtree.bounds\n",
    "    partition_list = []\n",
    "    queue = deque()\n",
    "    queue.append(mbr)\n",
    "    while len(queue):\n",
    "        br = queue.popleft()\n",
    "        nPoints = rtree.count(br)\n",
    "        if nPoints > maxPoints:\n",
    "            (br1, br2) = _reduced_boundary_split(rtree, br, eps)\n",
    "            queue.append(br1)\n",
    "            queue.append(br2)\n",
    "        else:\n",
    "            partition_list.append(br)\n",
    "    return partition_list\n",
    "def _reduced_boundary_split(rtree, br, eps):\n",
    "    (xmin, ymin, xmax, ymax) = br\n",
    "    \n",
    "    # vertical splitline candidates\n",
    "    ymin_score = float('inf')\n",
    "    ysplit = ymin + (ymax - ymin)/2\n",
    "    ybest_split = ((xmin, ymin, xmax, ysplit), (xmin, ysplit, xmax, ymax)) \n",
    "    while(ysplit + eps*2 <= ymax):\n",
    "        br1 = (xmin,ymin, xmax, ysplit)\n",
    "        br2 = (xmin, ysplit, xmax, ymax)\n",
    "        point_diff = abs(rtree.count(br1) - rtree.count(br2))\n",
    "        score = point_diff * rtree.count((xmin, ysplit-eps, xmax, ysplit+eps))\n",
    "        if score < ymin_score:\n",
    "            ymin_score = score\n",
    "            ybest_split = (br1, br2)\n",
    "            if rtree.count(br1) > rtree.count(br2):\n",
    "                ysplit = ymin + (ysplit - ymin)/2\n",
    "            else:\n",
    "                ysplit = ysplit + (ymax - ysplit)/2\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    # horizontal splitline candidates\n",
    "    xsplit = xmin + eps * 2\n",
    "    xmin_score = float('inf')\n",
    "    xbest_split = ((xmin, ymin, xsplit, ymax), (xsplit, ymin, xmax, ymax)) \n",
    "    while( xsplit + eps * 2<= xmax):\n",
    "        br1 = (xmin , ymin, xsplit, ymax)\n",
    "        br2 = (xsplit, ymin, xmax, ymax)\n",
    "        point_diff = abs(rtree.count(br1) - rtree.count(br2))\n",
    "        score = point_diff * rtree.count((xmin - eps, ymin, xmin + eps, ymax))\n",
    "        if score < xmin_score:\n",
    "            xmin_score = score\n",
    "            xbest_split = (br1, br2)\n",
    "            if rtree.count(br1) > rtree.count(br2):\n",
    "                xsplit = xmin + (xsplit - xmin)/2\n",
    "            else:\n",
    "                xsplit = xsplit + (xmax - xsplit)/2\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if xmin_score < ymin_score:\n",
    "        return xbest_split\n",
    "    else:\n",
    "        return ybest_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct rtree index\n",
    "\n",
    "def construct_rtree_index(dataset):\n",
    "    p = index.Property()\n",
    "    rtree_idx = index.Index(properties=p)\n",
    "    count = 0\n",
    "    for coordinate in dataset:       \n",
    "        rtree_idx.insert(count,(*coordinate, *coordinate))\n",
    "        count+=1        \n",
    "    return rtree_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rtree_partition(dataset):\n",
    "    idx = construct_rtree_index(dataset)\n",
    "    # #split test\n",
    "    cost_based = cost_base_partition(idx, 1000000, 50)\n",
    "    reduced_boundary = reduced_boundary_partition(idx, 500, 50)\n",
    "\n",
    "    partitioned = cost_based\n",
    "\n",
    "    indexed_data = []\n",
    "    id_ptt = 0\n",
    "    for boundary in partitioned:    \n",
    "        (left, bot, right, top) = boundary\n",
    "        for id_pts in idx.intersection((left-eps, bot-eps, right+eps, top+eps )):\n",
    "            indexed_data.append([id_ptt, id_pts])\n",
    "        id_ptt+=1\n",
    "\n",
    "    res = sc.parallelize(indexed_data).groupByKey().map(lambda x: [x[0], list(x[1])])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
